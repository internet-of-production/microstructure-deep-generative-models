# -*- coding: utf-8 -*-
"""
Created on Tue Aug 24 13:06:07 2021

@author: Schenk
"""
from __future__ import print_function

# Import standard library dependencies
import os
import argparse
from math import pi
from os import listdir
from os.path import isfile, join
import seaborn as sns

import cv2
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
import torch.nn.parallel
import torch.utils.data
from openpyxl import load_workbook
from skimage import measure, util
from skimage.color import rgb2gray, rgba2rgb
from skimage.filters import gaussian, threshold_otsu
from skimage.io import imread
from skimage.measure import label

def analyze_training_data(configdata, metrics, plotoption=True):
    """Compute and save evaluations metrics describing properties of training
    data images. These are used for comparison with the properties of images
    synthetically generated by generative machine learning models.

    Parameters
    ----------
    dataroot : str
        Path to the directory containing subfolders with training data images.
    metrics : dict
        A dictionary specifying which metrics to compute. Usable metrics are:
        perimeter, max_feret, min_feret and orientation. Dictionary is of form
        metrics["Skimage_Parameters"][<metric>]. Metrics are computed if the
        value of a respective key is set to "on". Shape_Factor is always
        evaluated. For more informatio on the metrics, refer to scikit-image's
        documentation.
    plotoption : bool, optional
        Boolean specifying whether or not Shape_Factor is plotted, by default
        True
    """
    pixelsize=configdata["get_data_loaders"]["real_size"]/configdata["get_data_loaders"]["image_size"]
    dataroot=configdata["get_data_loaders"]["dataroot"]

    dataroot=dataroot+'/Training'
    
    # Get column names of all metrics to be computed for DataFrame with results
    tablecolumns = resulttable(metrics)

    # Create list wwith paths to all subfolders in data root directory
    subfolders = [f.path for f in os.scandir(dataroot) if f.is_dir()]

    #set iterator to 1, each variant indicates a different label
    variant = 1  

    if "Micrograph" in metrics["Image Type"]:
        averaging=1
    else:
        averaging=0
        
    for i in subfolders:

        # Create list with paths to images in subfolder i
        images = [f for f in listdir(i) if isfile(join(i, f))]

        # Create empty DataFrame to store results
        imagelist = list(range(1, len(images)+1))
        df = pd.DataFrame(index=imagelist, columns=tablecolumns)

        numdata=0

        # Iterate over all images in subfolder i and compute evaluation metrics
        print  (f'Start reading {i}')
        
        for image in images:
            
            #check if file is an image
            if image.split(".")[-1]=='png' or image.split(".")[-1]=='jpg':
                location = os.path.join(f"{i}", f"{image}")
                rgba = imread(location)
                imagesize = rgba.shape[0]
    
                if rgba.shape[-1] == 4:
                    rgb = rgba2rgb(rgba) * 255
                else:
                    rgb = rgba * 255
    
                # Binarize image
                original = rgb2gray(rgb)
                inverted = util.invert(original)
                blurred = gaussian(inverted, sigma=.8)
                
                if ((np.amax(blurred)-np.amin(blurred))>0):
                    binary = blurred > threshold_otsu(blurred)
                    
                    #pass binary image to analyzing function
                    result=analyze_image(binary, tablecolumns,averaging,imagesize,image,pixelsize)            
    
                    if len(result)>0:
                        numdata+=1
                        for j in range(len(result)):
                            df.at[numdata, tablecolumns[j]] = result[j]
                        if numdata%1==0:
                            print (f'Image Number {numdata}')

        # Plot distribution of Shape_Factor (optional)
        if plotoption:
            plt.hist(df["Shape_Factor"], density=True, bins=30)  # density=False would make counts
            plt.ylabel('Probability')
            plt.xlabel('Data')

        # Export data to excel sheet
        resultfile = configdata["get_data_loaders"]["Analysis_File"]
        
        #If excel sheet does not exist yet, then...
        if variant > 1:
            book = load_workbook(resultfile)
            
        writer = pd.ExcelWriter(resultfile, engine='openpyxl')
        
        #If excel sheet does not exist yet, then...
        if variant > 1:
            writer.book = book
            
        sheetname = i.split("\\")[-1]
        df.to_excel(
            writer,
            sheet_name=sheetname,
            columns=tablecolumns,
            startrow=0,
            header=True,
            index=False
        )
        worksheet = writer.sheets[sheetname]

        # Get the dimensions of the dataframe.
        (max_row, max_col) = df.shape
        # Create a list of column headers, to use in add_table().
        column_settings = [{'header': column} for column in df.columns]
        writer.save()
        del df
        #Increase iterator
        variant += 1


def analyze_samples(Generator, configdata, metrics, Resultfolder, labeldic,
                    label=0, currentepoch=0):
    """Compute and save evaluation metrics describing properties of images
    generated by generative model. Additionally, plot distribution for each
    metric over training data and generated data to evaluate to what extend
    generated images represent the distribution of the real images.

    Parameters
    ----------
    Generator : torch.nn.Module
        Generator model (currently only supports generator of a GAN)
    configdata : dict
        Configuration dictionary specifying the dimensionality of the 
        generators random latent vector (configdata["Generator"]["nz"]; int),
        the size of the images (configdata["get_data_loaders"]["image_size"];
        int), whether the generator is used on GPU or CPU
        (configdata["Discriminator"]["device"]; str, torch.device), and the
        number of images to be generated (metrics["Num_images"]; int).
    metrics : dict
        A dictionary specifying which metrics to compute. Usable metrics are:
        perimeter, max_feret, min_feret and orientation. Dictionary is of form
        metrics["Skimage_Parameters"][<metric>]. Metrics are computed if the
        value of a respective key is set to "on". Shape_Factor is always
        evaluated. For more informatio on the metrics, refer to scikit-image's
        documentation.
    Resultfolder : str
        Path to a directory in which the results are to be saved.
    labeldic : dict
        Dictionary matching labels used by the model (keys) with human-readable
        labels (values). For example: {0: "0_Pores_Fine"; [...]}.
    label : int, optional
        Label (respectively the conditional value) for which images are to be
        generated, by default 0
    currentepoch : int, optional
        Current training epoch, by default 0
    """

    nz = configdata["Generator"]["nz"]
    imagesize = configdata["get_data_loaders"]["image_size"]
    device = configdata["Discriminator"]["device"]
    num_images = metrics["Num_images"]
    subfolder = os.path.join(Resultfolder, f"Epoche_{currentepoch}")
    pixelsize=configdata["get_data_loaders"]["real_size"]/configdata["get_data_loaders"]["image_size"]
    # Create directory to save results
    subsubfolder = os.path.join(subfolder, f"Label_{label}")
    os.makedirs(subsubfolder, exist_ok=True)

    # Get column names of all metrics to be computed for DataFrame with results
    tablecolumns = resulttable(metrics)

    # Create empty DataFrame for to store metrics
    imagelist = list(range(1, num_images + 1))
    df = pd.DataFrame(index=imagelist, columns=tablecolumns)
    
    if "Micrograph" in metrics["Image Type"]:
        averaging=1
    else:
        averaging=0
        
    for i in range(1, num_images + 1):
        
        #Generate images
        noise = torch.randn(1, nz, device=device)
        noise = noise.to(configdata["Discriminator"]["device"])
        
        label_vec=torch.IntTensor([label])
        label_vec=label_vec.to(configdata["Discriminator"]["device"])
     
        plotlabel='Generated'
        original = Generator.forward(noise,label_vec).detach().cpu().numpy().reshape(imagesize, imagesize)*255
            
        # Binarize image
        inverted = util.invert(original)
        blurred = gaussian(inverted, sigma=.8)
        binary = blurred > threshold_otsu(blurred)

        #pass binary image to analyzing function
        result=analyze_image(binary, tablecolumns,averaging,imagesize,str(i),pixelsize)            

        if len(result)>0:
            for j in range(len(result)):
                df.at[i, tablecolumns[j]] = result[j]

    # Compare with training data
    trainingdata = configdata["get_data_loaders"]["Analysis_File"]
    
    if not os.path.isfile(trainingdata):
        analyze_training_data(
            configdata, metrics
        )
    sheetname = labeldic[str(label)]
    training_data = pd.read_excel(
        trainingdata, sheet_name=sheetname, header=0
    )
    
    Imagefolder= os.path.join(Resultfolder, f"Epoche_{currentepoch}", f"Label_{label}")

    export_result_images(configdata,tablecolumns,Imagefolder,['Training',plotlabel],training_data,df)

    # Export data to excel sheet;
    export_to_excel(Resultfolder,currentepoch,label,df)

def analyze_image(binary, tablecolumns,averaging,imagesize,image,pixelsize):
    # Assign labels to connected regions in image and measure
    # properties of labelled regions
    labels = measure.label(binary)
    
    # Get properties using skimage
    props = measure.regionprops(labels)

    # initialize values that track the analysis 
    shapefactor = 0
    result = []
    line=0
    
    #isvalid is turned to 1 as soon as the microstructure can be evaluated
    isvalid=0
    
    #averaging is used for micrographs. For single pores, no averaging is used
    if averaging:
        #set up arrays to store values of single pores in case of microstructure analysis
         #set up output variable
        output=argparse.Namespace()
        output.perimeter=0
        output.major_axis_length=0
        output.minor_axis_length=0
        output.orientation=0
        output.shape=0
        output.solidity=0
        perimeter=np.zeros(len(props))
        major_axis_length=np.zeros(len(props))
        minor_axis_length=np.zeros(len(props))
        orientation=np.zeros(len(props))
        solidity=np.zeros(len(props))
        area=np.zeros(len(props))
        total_area=np.zeros(len(props))
        
    #iterate over all pores that are found in the image
    for pore in props:   
        minr, minc, maxr, maxc =pore.bbox     
        #check if pores are at the boundary, then analysis is not sensible
        
        if ((minr>0)&(minc>0)&(maxr<imagesize)&(maxc<imagesize)): 
            
            #check if poreanalysis has found sensible values
            if ((pore.area>0)&(pore.perimeter>0)):
                
                #if pore can be evaluated set isvalid to 1
                isvalid=1
                
                #if micrograph is evaluated, the pore values are stored in a list
                if averaging:
                    perimeter[line]=pore.perimeter
                    major_axis_length[line]=pore.major_axis_length
                    minor_axis_length[line]=pore.minor_axis_length
                    orientation[line]=pore.orientation
                    solidity[line]=pore.solidity
                    area[line]=pore.area
                    line+=1
                    
                #if single pore is evaluated, the results are directly passed as results
                else:
                    output=pore
                    
    #continue if analysis is valid
    if isvalid:
        
        #for micrographs, do averaging depending on the area of each pore
        if averaging:
            major_axis_length*=area
            minor_axis_length*=area
            orientation*=area
            solidity*=area
            shape=(4*pi*area[0:line]**2)/(perimeter[0:line]**2)
            perimeter*=area
            output.perimeter=sum(perimeter[0:line])/sum(area[0:line])
            output.major_axis_length=sum(major_axis_length[0:line])/sum(area[0:line])
            output.minor_axis_length=sum(minor_axis_length[0:line])/sum(area[0:line])
            output.orientation=sum(orientation[0:line])/sum(area[0:line])
            output.solidity=sum(solidity[0:line])/sum(area[0:line])
            shapefactor=sum(shape[0:line])/sum(area[0:line])
            
        else:
            shapefactor=4*pi*output.area/(output.perimeter**2)
        
        #save the results
        result.append(str(image))
        result.append(shapefactor)    
        if 'Perimeter' in tablecolumns:
            result.append(output.perimeter*pixelsize)
        if 'Max_Feret' in tablecolumns:
            result.append(output.major_axis_length*pixelsize)
        if 'Min_Feret' in tablecolumns:
            result.append(output.minor_axis_length*pixelsize)
        if 'Orientation' in tablecolumns:
            result.append(output.orientation*180/pi)
        if 'Porosity' in tablecolumns:
            result.append(np.mean(binary))
        if 'Solidity' in tablecolumns:
            result.append(output.solidity)
        if 'Curvature_Positive' in tablecolumns:
            result.append(curvature_analysis(binary,pixelsize,1))
        if 'Curvature_Negative' in tablecolumns:
            result.append(curvature_analysis(binary,pixelsize,2))
    return result

def resulttable(metrics):
    tablecolumns = ['Image_Number', 'Shape_Factor']
    if metrics["Skimage_Parameters"]["perimeter"] == "on":
        tablecolumns.append('Perimeter')
    if metrics["Skimage_Parameters"]["max_feret"] == "on":
        tablecolumns.append('Max_Feret')
    if metrics["Skimage_Parameters"]["min_feret"] == "on":
        tablecolumns.append('Min_Feret')
    if metrics["Skimage_Parameters"]["orientation"] == "on":
        tablecolumns.append('Orientation')
    if metrics["Skimage_Parameters"]["porosity"] == "on":
        tablecolumns.append('Porosity')
    if metrics["Skimage_Parameters"]["solidity"] == "on":
        tablecolumns.append('Solidity')
    if metrics["Skimage_Parameters"]["curvature_pos"] == "on":
        tablecolumns.append('Curvature_Positive')
    if metrics["Skimage_Parameters"]["curvature_neg"] == "on":
        tablecolumns.append('Curvature_Negative')
    return tablecolumns

def export_to_excel(Resultfolder,currentepoch,label,df):
    
    filename="Generated_Images.xlsx" 
        
    excelfile=os.path.join(Resultfolder, f"Epoche_{currentepoch}", f"Label_{label}", filename)
    writer = pd.ExcelWriter(excelfile, engine='xlsxwriter')
    df.to_excel(
        writer,
        sheet_name='Sheet1',
        startrow=1,
        header=False,
        index=False
    )
    workbook = writer.book
    worksheet = writer.sheets['Sheet1']
    # Get the dimensions of the DataFrame.
    (max_row, max_col) = df.shape
    # Create a list of column headers, to use in add_table().
    column_settings = [{'header': column} for column in df.columns]
    # Add the Excel table structure. Pandas will add the data.
    worksheet.add_table(
        0, 0, max_row, max_col - 1, {'columns': column_settings}
    )
    # Make the columns wider for clarity.
    worksheet.set_column(0, max_col - 1, 12)
    # Close the Pandas Excel writer and output the Excel file.
    writer.save()
    
def export_result_images(configdata,tablecolumns,Resultfolder,plotlabels,*argv):

    sns.set_style("white")

    # Iterate over all metrics in DataFrame (columns) and plot distribution
    # of the values per metric as histogram
    imagesize=configdata["get_data_loaders"]["image_size"]
    
    #If more than 2 input arrays are given, create a seperate folder for comparison (e.g. real with GAN with SLERP)
    if len(argv)>2:
        
        Resultfolder=os.path.join(Resultfolder, f"Comparison_{plotlabels[1]}_vs_{plotlabels[2]}")
        os.makedirs(Resultfolder, exist_ok=True)
        
    for i in range(1, len(tablecolumns)):

        # Define bin edges and min and max value of x-axis for histograms
        if tablecolumns[i] == "Shape_Factor":
            bins = np.linspace(0, 1, 100)
            minval = 0
            maxval = 0.6
            labelx="Shape Factor"
        elif tablecolumns[i] == "Solidity":
            bins = np.linspace(0.5, 1, 100)
            minval = 0.4
            maxval = 1.0
            labelx="Solidity"
        elif tablecolumns[i] == "Porosity":
            bins = np.linspace(0, 1, 100)
            minval = 0
            maxval = 0.20  
            labelx=tablecolumns[i]
        elif tablecolumns[i] == "Curvature_Positive":
            minval = 0
            maxval = 4 
            bins = np.linspace(minval,maxval, 100)
            labelx="Concave curvature [$\mu m^{-1}$] "
        elif tablecolumns[i] == "Curvature_Negative": 
            minval = -8
            maxval = -2 
            bins = np.linspace(minval,maxval, 100)
            labelx="Convex curvature [$\mu m^{-1}$] "
        else:
            minval = np.floor(
                np.amin(argv[0][tablecolumns[i]])/10
            )*10
            maxval = np.ceil(
                np.amax(argv[0][tablecolumns[i]])/10
            )*10
            for arg in argv:
                minvalinput = np.floor(np.amin(arg[tablecolumns[i]])/10)*10
                minval = min(minval, minvalinput)
                maxvalinput = np.ceil(np.amax(arg[tablecolumns[i]])/10)*10
                maxval = max(maxval, maxvalinput)
            bins = np.linspace(minval, maxval, 100)
            labelx=str.replace(tablecolumns[i],'_',' ')
            if tablecolumns[i] == "Orientation":
                minval = -90.0
                maxval = 90.0 
                labelx=labelx
            else:
                labelx=labelx+r' [$\mu$m]'

        kwargs = dict(hist_kws={'alpha':.6}, kde_kws={'linewidth':2})       
        plt.figure(figsize=(512/96, 512/96), dpi=96)
        fig1 = plt.gcf()
        x=0
        colors=['blue','red','green']
        
        for arg in argv:
            sns.distplot(arg[tablecolumns[i]],color=colors[x], label=[plotlabels[x]], **kwargs)
            x+=1


        plt.legend(loc='upper right')
        plt.ylabel('Probability')
        plt.xlabel(labelx)

        plt.xlim([minval, maxval])
        import tikzplotlib
        name_tex=os.path.join(Resultfolder, "Histogram_V2_"+str.replace(tablecolumns[i],'_',' ')+".tex")
        name_png=os.path.join(Resultfolder, f"Histogram_V2_{tablecolumns[i]}.png")
        tikzplotlib.save(name_tex)
        fig1.savefig(name_png)
        plt.close()
        
def curvature_analysis(image,pixelsize,mode): 

   #Compute distance function
   img=np.array(image)
   labelled=label(image)
   pore=np.uint8(img*1)
   output_pore=cv2.distanceTransform(np.uint8(pore), cv2.DIST_L2, 5)
   particle=np.invert(image)
   particle=np.uint8(particle*1)
   output_particle=cv2.distanceTransform(np.uint8(particle), cv2.DIST_L2, 5)
   output=output_pore-output_particle

   #Compute curvature
   curv=get_curvature(output,pixelsize)
   filtered_curv=curv*(output>0)*(output<np.sqrt(2))
   
   max_curv=[]
   min_curv=[]
   
   for n in range(1,np.amax(labelled)):
       label_curv=filtered_curv*(labelled==n)
       max_curv.append(np.amax(label_curv))
       min_curv.append(np.amin(label_curv))
       
   result_pos=np.mean(max_curv)
   result_neg=np.mean(min_curv)
   
   #Return negative or positive curvature
   if mode==1:
       result=result_pos
   else:
       result=result_neg
       
   return result      

def shift(array,dim,value):
    
    shifted=np.zeros(np.shape(array))
    if dim==1:
        #Shift right
        if value>0:
            shifted[:,0]=array[:,-1]
            shifted[:,1:]=array[:,0:np.shape(array)[1]-1]
        #Shift left
        elif value<0:
            shifted[:,-1]=array[:,0]
            shifted[:,0:np.shape(array)[1]-1]=array[:,1:]
    elif dim==2:
        #Shift downwards
        if value>0:
            shifted[0,:]=array[-1,:]
            shifted[1:,:]=array[0:np.shape(array)[0]-1,:]
        #Shift upwards
        elif value<0:
            shifted[-1,:]=array[0,:]
            shifted[0:np.shape(array)[0]-1,:]=array[1:,:]
    return shifted

def get_curvature(array,pixelsize):
    
    #Compute curvature as the divergence of the distance function
    xminus=shift(array,1,1)
    xplus=shift(array,1,-1)
    yminus=shift(array,2,-1)
    yplus=shift(array,2,1)
    dxp=xplus-array
    dxm=array-xminus
    dyp=yplus-array
    dym=array-yminus
    dx=(abs(dxp)>=abs(dxm))*dxp+(abs(dxm)>abs(dxp))*dxm
    dx[:,0]=dxp[:,0]
    dx[:,-1]=dxm[:,-1]
    
    dy=(abs(dyp)>=abs(dym))*dyp+(abs(dym)>abs(dyp))*dym
    dy[0,:]=dym[0,:]
    dy[-1,:]=dyp[-1,:]
    len_norm=np.sqrt(dx*dx+dy*dy)
    len_norm[np.isnan(len_norm)] = 1
    dx/=len_norm
    dy/=len_norm
    
    xxminus=shift(dx,1,1)
    xxplus=shift(dx,1,-1)
    yyminus=shift(dy,2,-1)
    yyplus=shift(dy,2,1)
    
    ddx=(xxplus-xxminus)/2
    ddy=(yyplus-yyminus)/2
    
    return((ddx+ddy)/pixelsize)